package com.nanthealth.ingest

import org.apache.log4j.LogManager
import org.apache.log4j.Logger
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.functions.callUDF
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.DataFrame
import java.util.Properties
import java.io.File
import java.io.FileInputStream
import java.io.InputStreamReader
import scala.io.Source
import java.sql.DriverManager
import java.sql.Connection
import scala.collection.mutable.ListBuffer
import scala.collection.mutable.Map
import org.apache.hadoop.conf._
import org.apache.hadoop.fs._

object HdfsIngest {
  private val logger = LogManager.getLogger(getClass.getName)

  def main(args: Array[String]) : Unit = {

    if (args.length < 1) {
      println("Database name is required as parameter...")
      System.exit(1)
    }

    var filepath : String = null
    val prop = new Properties()


    if (args.length == 2) {
      filepath = "hdfs://nameservice1" + args(1)
      val pt = new Path(filepath)
      val conf = new org.apache.hadoop.conf.Configuration()
      val filesystem = FileSystem.get(conf)
      val fis = new InputStreamReader(filesystem.open(pt))
      prop.load(fis)
    } else {
      filepath = "ingest.config"
      val fis = new FileInputStream(new File(filepath))
      prop.load(fis)
    }

    val (user, url, password, driver, csv_path, hive_db) =
      try {
        (
          prop.getProperty("user"),
          prop.getProperty("url") + args(0),
          prop.getProperty("password"),
          prop.getProperty("driver"),
          prop.getProperty("csv_path"),
          args(0) + "_ingest"
        )
      } catch { 
         case e: Exception => e.printStackTrace(); logger.error(e); sys.exit(1);
      }

    val conf = new SparkConf().setAppName("DataLake Data Ingestion")
    val sc = new SparkContext(conf)
    val hiveContext = new HiveContext(sc)

    var connection:Connection = null
    var options = Map[String, String]()
    options += ("url" -> url)
    options += ("user" -> user)
    options += ("password" ->  password)
    options += ("driver" -> driver)

    hiveContext.udf.register("CleanseCol", cleanCol(_ : String))

    try {
      Class.forName(driver)
      connection = DriverManager.getConnection(url, user, password)

      val statement = connection.createStatement()
      val resultSet = statement.executeQuery("SELECT name FROM sys.tables")
      while ( resultSet.next() ) {
        val table = resultSet.getString("name")

        options += ("dbtable" ->  table)

        val df = hiveContext.read.format("jdbc").options(options).load

        val dft = transform(df)

//        dft.write
//           .format("com.databricks.spark.csv")
//           .option("header", "true")
//           .option("nullValue", "")
//           .save(csv_path + "/" + table.toLowerCase())

        dft.write
           .mode(SaveMode.Overwrite)
           .saveAsTable(hive_db + "." + table.toLowerCase());
      }
    } catch {
      case e : Throwable => e.printStackTrace; logger.error(e);
    }

    connection.close

    sc.stop

  } //:~ main


  def cleanCol (content : String) : String = {
    var col : String = null
    if (content != null) {
      col = content.replace("\n", " ").replace("\r", " ") 
    }
    col
  }
  

  def transform(input : DataFrame) : DataFrame = {
    var df = input
    for (tup <- df.dtypes) {
      if (tup._2.equalsIgnoreCase("StringType")) {
        val columnName = tup._1
        df = df.withColumn(columnName, callUDF("CleanseCol", df.col(columnName)))
      }
    }
    df.toDF
  } //:~ transform

}
